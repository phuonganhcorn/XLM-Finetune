# XLM-Finetune-QLoRA for Vietnamese Language
#### Author: Ngo Phuong Anh


> [!NOTE]
> - This model is an optimized version of XLMRoBERTa model for finetuning it for QA task by applied LoRA/QLoRA into training process.
> - This project is inspired by a state-of-art Mistral7B Large Language model and a Finetuned XLMRoBERTa Version for Vietnamese Question Answering made by Nguyen Vu Le Binh.


### Overview
- XLM-RoBERTa is a pre-trained cross-lingual language model developed by Facebook AI. It combines elements from two popular models, XLM (Cross-Lingual Language Model) and RoBERTa (Robustly optimized BERT approach), to create a model that is capable of understanding and generating text in multiple languages. 
- The core architecture of XLM-RoBERTa is built based on Transformer architecture with Attention Mechanism. Made it better than other RNN, LTSM model for NLP tasks.

